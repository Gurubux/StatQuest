{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load our data into a dataframe\n",
    "import pandas\n",
    "events = pandas.read_csv('connected_cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_datetime(date_str, time_str):\n",
    "    '''\n",
    "    Date should be a string in format YYYY/MM/DD, and time should \n",
    "    be a string in format HH:MM:SS (24 hour time).  Together, they\n",
    "    should specify a time in UTC.\n",
    "    \n",
    "    Returns a datetime object relative to PDT.\n",
    "    \n",
    "    If the format of either date_str or time_str is invalid, returns null.\n",
    "    '''\n",
    "    try:\n",
    "        utc_date = datetime.strptime(date_str, '%Y/%m/%d').date()\n",
    "        utc_time = datetime.strptime(time_str, '%H:%M:%S').time()\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "    utc_dt = datetime.combine(utc_date, utc_time)\n",
    "    pdt_dt = utc_dt - timedelta(hours=7) # UTC is 7 hours ahead of PDT\n",
    "    return pdt_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update the events dataframe to have parsed dates and times relative to PST\n",
    "parsed_datetimes = [parse_datetime(date, time) for date, time in zip(events['date'], events['time'])]\n",
    "events['date'] = [dt and dt.date() for dt in parsed_datetimes]\n",
    "events['time'] = [dt and dt.time() for dt in parsed_datetimes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove rows of the dataframe with invalid dates or times\n",
    "events = events[events['date'].apply(lambda d: d is not None)]\n",
    "events = events[events['time'].apply(lambda d: d is not None)]\n",
    "events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect statistics on the sampling frequency of our data\n",
    "\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "\n",
    "def get_sampling_intervals(person_id):\n",
    "    # Calculate sampling intervals per person and date\n",
    "    person_events = events[events['driver_id'] == person_id]\n",
    "    \n",
    "    sampling_intervals = []\n",
    "    for date, group in person_events.groupby(['date']):\n",
    "        times = sorted(group['time'])\n",
    "        for time1, time2 in zip(times, islice(times, 1, None)):\n",
    "            interval = datetime.combine(date, time2) - datetime.combine(date, time1)\n",
    "            # Record the interval in minutes\n",
    "            sampling_intervals.append(interval.seconds / 60.)\n",
    "            \n",
    "    return sampling_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the unique people in our dataset\n",
    "import numpy\n",
    "people = numpy.unique(events['driver_id'])\n",
    "len(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sampling_intervals = [i for p in people for i in get_sampling_intervals(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make plots appear inline\n",
    "%pylab inline --no-import-all \n",
    "\n",
    "# Importing this makes plots more visually pleasing\n",
    "import seaborn\n",
    "\n",
    "# Plot the sampling frequency\n",
    "plt.plot(all_sampling_intervals)\n",
    "plt.ylabel('Inter-sampling time (min)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_sampling_intervals(sampling_intervals, person_id=None):\n",
    "    # Range determines the range used to divide the bins, and xlim determines\n",
    "    # the range of the axis labels\n",
    "    plt.hist(sampling_intervals, bins=15, log=True, range=(0,900))\n",
    "    plt.xlim((0, 900))\n",
    "    person_message = ' for Person {}'.format(person_id) if person_id is not None else ''\n",
    "    plt.xlabel('Inter-sampling time (min){}'.format(person_message))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the sampling frequency for everyone\n",
    "plot_sampling_intervals(all_sampling_intervals)\n",
    "\n",
    "# Plot the sampling frequency for each person separately\n",
    "for person_id in sorted(set(events['driver_id'].values)):\n",
    "    sampling_intervals = get_sampling_intervals(person_id)\n",
    "    plot_sampling_intervals(sampling_intervals, person_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(events['x'], events['y'], marker='o', alpha=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Restrict events to the region -1 to 1 in both x and y dimensions\n",
    "restricted_region = events[events['x'] >= -1]\n",
    "restricted_region = restricted_region[restricted_region['x'] <= 1]\n",
    "restricted_region = restricted_region[restricted_region['y'] >= -1]\n",
    "restricted_region = restricted_region[restricted_region['y'] <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a scatterplot of all locations\n",
    "plt.scatter(restricted_region['x'], restricted_region['y'], marker='o', alpha=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the location of each person on a separate plot\n",
    "plt.clf()\n",
    "for person in people:\n",
    "    person_events = events[events['driver_id'] == person]\n",
    "    plt.scatter(person_events['x'], person_events['y'], marker='o', alpha=0.03)\n",
    "    plt.title('Location of person {}'.format(person))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the location of each person within the restricted region\n",
    "plt.clf()\n",
    "for person in people:\n",
    "    person_events = restricted_region[restricted_region['driver_id'] == person]\n",
    "    plt.scatter(person_events['x'], person_events['y'], marker='o', alpha=0.03)\n",
    "    plt.title('Location of person {}'.format(person))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between two points\n",
    "import math\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "distance_metric = euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate each person's distance from the origin at 4 pm, 5 pm, 6 pm, and 7 pm every day\n",
    "# The distances will be our feature vectors, X\n",
    "# The people will be our classes, y\n",
    "\n",
    "# Then we can try to predict which of our two selected people a day's worth\n",
    "# of data belongs to.\n",
    "\n",
    "from bisect import bisect_left\n",
    "from datetime import time\n",
    "\n",
    "origin_x = 0\n",
    "origin_y = 0\n",
    "\n",
    "times_to_measure = [time(16, 0, 0), time(17, 0, 0), time(18, 0, 0), time(19, 0, 0)] #, time(23, 59, 0)]\n",
    "\n",
    "def calculate_points(events):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Go through each person and day in the data\n",
    "    for (person_id, date), event_group in events.groupby(['driver_id', 'date']):\n",
    "        event_group = event_group.sort('time')\n",
    "        distances = []\n",
    "        \n",
    "        for time in times_to_measure:\n",
    "            # Find the location at each time by finding the earliest \n",
    "            # reported location at or after that time\n",
    "            i = bisect_left(event_group['time'].values, time)\n",
    "            \n",
    "            # If no location was reported at or after the given time, just use\n",
    "            # the latest reported location\n",
    "            if i == event_group['time'].count():\n",
    "                i -= 1\n",
    "                \n",
    "            event = event_group.iloc[i]\n",
    "            dist = distance_metric(event['x'], event['y'], origin_x, origin_y)\n",
    "            distances.append(dist)\n",
    "            \n",
    "        X.append(distances)\n",
    "        y.append(person_id)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for person, event_group in events.groupby(['driver_id']):\n",
    "    print('Person {}: num events = {}'.format(person, len(numpy.unique(event_group['date']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose two people for our classification problem\n",
    "person1 = 'B'\n",
    "person2 = 'F'\n",
    "\n",
    "classification_events = events[events['driver_id'].apply(lambda p: p == person1 or p == person2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = calculate_points(classification_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make our results reproducible\n",
    "from numpy import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select a training set and a test set\n",
    "\n",
    "def split_train_test(X, y, train_percent=0.6):\n",
    "    split_index = int(len(X) * train_percent)\n",
    "    \n",
    "    # Combine features and labels into a single list so we can shuffle together\n",
    "    points = zip(X, y)\n",
    "    random.shuffle(points)\n",
    "    training_points = points[:split_index]\n",
    "    test_points = points[split_index:]\n",
    "    \n",
    "    \n",
    "    # Now that our points have been randomly ordered, separate the features from the labels again\n",
    "    # Also put them in numpy arrays.\n",
    "    X_train = numpy.array([X_point for (X_point, y_point) in training_points])\n",
    "    y_train = numpy.array([y_point for (X_point, y_point) in training_points])\n",
    "    \n",
    "    X_test = numpy.array([X_point for (X_point, y_point) in test_points])\n",
    "    y_test = numpy.array([y_point for (X_point, y_point) in test_points])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot our features within our training data\n",
    "\n",
    "person1_features = []\n",
    "person2_features = []\n",
    "\n",
    "for X_point, y_point in zip(X_train, y_train):\n",
    "    if y_point == person1:\n",
    "        person1_features.append(X_point)\n",
    "    else:\n",
    "        person2_features.append(X_point)\n",
    "        \n",
    "def plot_features(person, person_features):\n",
    "    for i, time in enumerate(times_to_measure):\n",
    "        features = [point[i] for point in person_features]\n",
    "        plt.hist(features, bins=20)\n",
    "        plt.title('Distribution of distance at {} for Person {}'.format(time, person))\n",
    "        plt.show()\n",
    "    \n",
    "plot_features(person1, person1_features)\n",
    "plot_features(person2, person2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up three models to use for our classification problem\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def init_models(X_train, y_train):\n",
    "    models = [LogisticRegression(),\n",
    "              RandomForestClassifier(),\n",
    "              SVC(probability=True)] # Enable the use of predict_proba later\n",
    "  \n",
    "    # Fit each model using the training data\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = init_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print accuracy rate for each classifier on the test data\n",
    "# (that is, the true positive rate plus the true negative rate)\n",
    "def name(model):\n",
    "    return model.__class__.__name__\n",
    "\n",
    "for model in models:\n",
    "    print('accuray for {}: {}'.format(name(model), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to plot a learning curve.\n",
    "# Taken from http://scikit-learn.org/dev/_downloads/plot_learning_curve.py\n",
    "\n",
    "def plot_learning_curve(estimator, plot_title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5),\n",
    "                        scoring=None):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        null for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"    \n",
    "    from sklearn.learning_curve import learning_curve\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(plot_title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Cost = 1 - Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y,\n",
    "                                                            cv=cv, n_jobs=n_jobs,\n",
    "                                                            train_sizes=train_sizes,\n",
    "                                                            scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, 1 - train_scores_mean + train_scores_std,\n",
    "                     1 - train_scores_mean - train_scores_std, alpha=0.1,\n",
    "                     color=\"b\")\n",
    "    plt.fill_between(train_sizes, 1 - test_scores_mean + test_scores_std,\n",
    "                     1 - test_scores_mean - test_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, 1 - train_scores_mean, 'o-', color=\"b\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, 1 - test_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Test score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot learning curve - requires bleeding-edge sklearn\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "for model in models:\n",
    "    cv = cross_validation.ShuffleSplit(len(X_train), n_iter=25, test_size=0.4)\n",
    "    plot_title = 'Learning Curves ({})'.format(name(model))\n",
    "    plot_learning_curve(model, plot_title, X_train, y_train, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print accuracy rate for each classifier on the test data\n",
    "# (that is, the true positive rate plus the true negative rate)\n",
    "def name(model):\n",
    "    return model.__class__.__name__\n",
    "\n",
    "for model in models:\n",
    "    print('accuray for {}: {}'.format(name(model), model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(model, X_test, y_test, pos_label=True):\n",
    "    y_pred = model.predict(X_test)\n",
    "       \n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for (pred, true) in zip(y_pred, y_test):\n",
    "        # Predicted positive and truly positive\n",
    "        if pred == pos_label and true == pos_label:\n",
    "            true_positive += 1\n",
    "        # Predicited positive but truly negative\n",
    "        elif pred == pos_label:\n",
    "            false_positive += 1\n",
    "        # Predicted negative but truly positive\n",
    "        elif true == pos_label:\n",
    "            false_negative += 1\n",
    "        # Predicted negative and truly negative\n",
    "        else:\n",
    "            true_negative += 1  \n",
    "            \n",
    "    row_fmt = '{:20s} | {:20s} | {:20s}'\n",
    "    print(row_fmt.format('', 'Condition positive', 'Condition negative'))\n",
    "    print(row_fmt.format('Test positive', str(true_positive), str(false_positive)))\n",
    "    print(row_fmt.format('Test negative', str(false_negative), str(true_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print('*********** {} ***********\\n'.format(name(model)))\n",
    "    print_confusion_matrix(model, X_test, y_test, pos_label=model.classes_[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = y_test\n",
    "    print('*********** {} ***********\\n'.format(name(model)))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classify person1 versus all other people rather than just person1 vs person2\n",
    "# ova stands for one vs. all\n",
    "ova_person = \"G\"\n",
    "events_subset = events[events['driver_id'].apply(lambda p: p in ['A', 'C', 'G', 'F'])]\n",
    "X_ova, y_intermediate = calculate_points(events_subset)\n",
    "y_ova = [label == ova_person for label in y_intermediate]\n",
    "X_train_ova, y_train_ova, X_test_ova, y_test_ova = split_train_test(X_ova, y_ova, train_percent=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_intermediate[220:230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ova[220:230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(y_train_ova).most_common())\n",
    "print(Counter(y_test_ova).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ova_models = init_models(X_train_ova, y_train_ova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in ova_models:\n",
    "    print('accuray for {}: {}'.format(name(model), model.score(X_test_ova, y_test_ova)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in ova_models:\n",
    "    print('*********** {} ***********\\n'.format(name(model)))\n",
    "    print_confusion_matrix(model, X_test_ova, y_test_ova)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve and print the area under the ROC curve\n",
    "# on the test data for each model\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(model, color, X_test, y_test):\n",
    "    probas = model.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,\n",
    "                                     probas[:, 0],\n",
    "                                     pos_label=model.classes_[0])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('Area under ROC for {}: {:.3f}'.format(name(model), roc_auc))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    # plot(fpr, tpr, color=color, label='{} \n",
    "    # ROC curve (area = {:.2f})'.format(name(model), roc_auc), linewidth=3)\n",
    "    spec = [ 1- x for x in fpr]\n",
    "    plt.plot(spec, tpr, color=color, label=name(model), linewidth=3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Specificity', fontsize=14)\n",
    "    plt.ylabel('Sensitivity', fontsize=14)\n",
    "    plt.legend(loc=0, prop={'size': 14})\n",
    "\n",
    "plt.clf()\n",
    "colors = [\"green\", \"blue\", \"red\"]\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    plot_roc_curve(model, color, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_precision_recall_curve(model, color, X_test, y_test):\n",
    "    probas = model.predict_proba(X_test)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test,\n",
    "                                                           probas[:, 1])\n",
    "    area = auc(recall, precision)\n",
    "    print('Area under curve for {}: {:.3f}'.format(name(model), area))\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision, label=name(model), linewidth=3)\n",
    "    plt.xlabel('Recall', fontsize=14)\n",
    "    plt.ylabel('Precision', fontsize=14)\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.legend(loc=0, prop={'size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model, color in zip(ova_models, colors):\n",
    "    plot_precision_recall_curve(model, color, X_test_ova, y_test_ova)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for model in ova_models:\n",
    "    print('*********** {} ***********'.format(name(model)))\n",
    "    print(classification_report(y_test_ova, model.predict(X_test_ova)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in ova_models:\n",
    "    print('{}: {}'.format(name(model), Counter(model.predict(X_test_ova)).most_common()))\n",
    "    \n",
    "print('truth: {}'.format(Counter(y_test_ova).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "for model in ova_models:\n",
    "    y_pred = model.predict(X_test_ova)\n",
    "    \n",
    "    print('*********** {} ***********'.format(name(model)))\n",
    "    print('F1 score: {}'.format(fbeta_score(y_test_ova, y_pred, 1)))\n",
    "    print('F2 score: {}'.format(fbeta_score(y_test_ova, y_pred, 2)))\n",
    "    print('F0.5 score: {}'.format(fbeta_score(y_test_ova, y_pred, 0.5)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
