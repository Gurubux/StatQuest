{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In which I implement Linear Regression on a sample data set from Andrew Ng's Machine Learning Course.\n",
    "\n",
    "Week 2 of Andrew Ng's ML course on Coursera focuses on doing linear regression with the common squared error cost function, using gradient descent for the actual fitting (remember to scale features!), and alternatively using the Normal Equation analytical solution. Here I use the homework data set to learn about the relevant python tools.\n",
    "\n",
    ">## Tools Covered:\n",
    "- `SGDRegressor` for general linear model regression specifying a loss and penalty\n",
    "- `LinearRegression` for normal equation solution to least squares loss linear regression\n",
    "- `mean_squared_error` for computing this super common loss metric\n",
    "- `StandardScaler` for feature scaling your data\n",
    "- also Surface Plots and Contour Plots in `matplotlib`\n",
    "\n",
    "\n",
    "# Visualizing the Data\n",
    "*Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next. The file `ex1data1.txt` contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.*\n",
    "\n",
    "*Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). (Many other problems that you will encounter in real life are multi-dimensional and can't be plotted on a 2-d plot.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snips'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4f3ea896a930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msnips\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'snips'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snips as snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd hw-wk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ex1data1.txt\", header=None)\n",
    "df.columns = [\"popn\", \"profit\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"popn\"], df[\"profit\"])\n",
    "snp.labs(\"population\", \"profit\", \"Initial Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is obviously a strong positive relationship which looks roughly linear though with a lot of scatter. There shouldn't be any need for feature scaling as both population and profit are reported in units that put them in the \"tens\" range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression With One Variable\n",
    "\n",
    "## Gradient Descent\n",
    "*In this part, you will fit the linear regression parameters to our dataset using gradient descent.*\n",
    "\n",
    "[According to the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) `scikit-learn`'s standard linear regression object is actually just a piece of code from `scipy` which is wrapped to give a predictor object. The aforementioned `scipy` function is **[`scipy.linalg.lstsq`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) which just uses the Normal equation to compute the minimizer analytically**, and will give you an warning if the relevant matrix is non-invertible! This is interesting as Andrew mentioned the normal equation almost as an afterthought to gradient descent as far as implementing a real algorithm for doing linear regression, but it seems in the real world the default method is perhaps the normal equation.   \n",
    "\n",
    "\n",
    "### Stochastic versus Batch Gradient Descent\n",
    "**If you really want to get gradient descent in `scikit-learn` then the relevant object is called `SGDRegressor`** (or for classification problems `SGDClassifier` - recall classification is where your labels are discrete category-like). The \"SGD\" stands for *stochastic* gradient descent, so it's not quite the same as what Andrew covers in his class, although he does discuss it in the real [Stanford CS299 notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf). On Coursera he mentions that using every data point in your training example to compute $\\frac{\\partial J}{\\partial \\theta_j}$ at every iteration of GD is called **batch gradient descent**, but he doesn't detail the alternatives, one of which is **stochastic gradient descent**. So allow me to quote from his real class notes:\n",
    "> In [stochastic gradient descent], we repeatedly run through the training set, and each time\n",
    "we encounter a training example, we update the parameters according to\n",
    "the gradient of the error with respect to that single training example only.\n",
    "\n",
    "According to [this SO answer](http://stackoverflow.com/questions/34469237/linear-regression-and-gradient-descent-in-scikit-learn-pandas) batch gradient descent is not often used in practice:\n",
    ">If you can decompose your loss function into additive terms, then stochastic approach is known to behave better... and if you can spare enough memory - OLS method is faster and easier. \n",
    "\n",
    "And for further reading on the difference between *batch* versus *stochastic* check out [this quora question](https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools in `sklearn` for Stochastic Gradient Descent\n",
    "With the `SGDRegressor` object you need to explicitly set your cost function to be squared error and tell it not to use any \"penalty\" (which Andrew hasn't discussed yet, but is essentially another way to influence the specific hypothesis which the ML algorithm returns). \n",
    "\n",
    "From the official documentation of `scikit-learn`:\n",
    "\n",
    ">Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with loss=\"log\", SGDClassifier fits a logistic regression model...\n",
    "\n",
    "\n",
    "The [official documention page for `SGD`](http://scikit-learn.org/stable/modules/sgd.html#sgd) is excellent and very clear. As mentioned above, this is a more general linear regression fitting object, so below I will outlineusing it with a focus on using it with squared error cost and using this HW data set as an example. \n",
    "\n",
    "First we need to instantiate the fitting object, telling it what our cost function AKA loss function is - in our case cost is squared error. We could also instantiate it with a penalty function here, but we're not using those so instead we set `None` since the default is actually `L2` penalty or \"regularization\" (don't worry about it for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create an SGDClassifier instance which will have methods to do our linear regression fitting by gradient descent\n",
    "fitter = SGDRegressor(loss=\"squared_loss\", penalty=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a fitting, the SGDClassifier object will want to see an array, $X$, holding your table of features for the training set, as well as a vector, $y$, holding the label i.e. y-value for each element of the training set. From our HW data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.as_matrix(columns=[\"popn\"])  # SGDRegressor is very particular on input \"X\" and insists on a true matrix \n",
    "y = df[\"profit\"].tolist() # This is the variable we want to predict, SGDRegressor expects a vector-like object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[A quick but important aside: in class when Andrew writes $X$ for the data matrix he usually means this to include a column of all ones for $\\theta_0$ (the intercept term). When using all these lovely `sklearn` tools it's not necessary to include this column explicitly in your $X$, it will be created for you.]*\n",
    "\n",
    "Now we actually do the fit AKA execute the gradient descent by calling a method of our `SGDRegressor` object. Note that calling this method will not return anything to do with the fit, but rather all the fitting information will be stored as attributes of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.fit(X, y)  # Do the fit by calling a method of our object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see some names that seem to correspond with Andrew's lectures such as `alpha` and `epsilon`. But actually `SGDRegressor` is just messing with you and all of these have different meanings from what Andrew talked about. Here's a couple \"gotchas\". Also this guy is implemented as a pretty high-level tool so it doesn't expose all the innards of the gradient descent is just performed. This means we can't, for example, plot the cost function $J$ as a function of number of iterations.\n",
    "\n",
    "At any rate, the fit is done and we can call the `predict` method of the fittng object to actually use the selected hypothesis in predicting new data. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(0, 26, 1))\n",
    "xs = np.array(xs).reshape(-1, 1)\n",
    "\n",
    "new_ys = fitter.predict(xs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"popn\"], df[\"profit\"])\n",
    "snp.labs(\"population\", \"profit\", \"Data vs. Linear Regression Gradient Descent Fit\")\n",
    "ax.plot(xs, new_ys, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does it Suck Out of the Box?\n",
    "By eye I'd say this did kind of a poopy job and a slightly smaller intercept with a slightly larger positive slope would improve \n",
    "our fit. What is the reason for this crappiness?  Well, [this SE](http://datascience.stackexchange.com/questions/6676/scikit-learn-getting-sgdclassifier-to-predict-as-well-as-a-logistic-regression) answer where someone ran into the same issue was super helpful in troubleshoting. It turns out [a good rule of thumb](http://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use) for the number of passes over the training set is `n_iter = np.ceil(10**6 / n)` to get convergence. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.n_iter = np.ceil(10**6 / len(y))\n",
    "fitter.fit(X, y)  # Do the fit by calling a method of our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(0, 26, 1))\n",
    "xs = np.array(xs).reshape(-1, 1)\n",
    "\n",
    "new_ys = fitter.predict(xs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"popn\"], df[\"profit\"])\n",
    "snp.labs(\"population\", \"profit\", \"Data vs. Linear Regression Gradient Descent Fit\")\n",
    "ax.plot(xs, new_ys, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, it looks like that was the problem. Using the default settings it was stopping the algorithm after only `n_iter = 5` runs through the full training set, which for this data set amounts to only 500 steps rather than the recommended 1 million!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Comparison to Normal Method\n",
    "For the sake of comparison, let's see how the normal equation method would do. Recall that the most commonly used linear regression tool in `sklearn` is the `LinearRegression` object, and it is actually using the normal method. The architecture of this class is super similar to what we just used with `SGDRegressor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "norm_eqn = LinearRegression()\n",
    "norm_eqn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ys = norm_eqn.predict(xs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"popn\"], df[\"profit\"])\n",
    "snp.labs(\"population\", \"profit\", \"Data vs. Linear Regression Gradient Descent Fit\")\n",
    "ax.plot(xs, new_ys, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, that's quite good and agrees pretty well with the optimized SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "As a sidenote, SGD is sensitive to feature scaling just like batch GD, so when needed you get to use [this super nifty toy](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guy has has `fit` method which, rather than fitting a model in the conventional sense, instead computes and saves the sample mean and variance for each feature in your input matrix `X` and then uses those values to drive it's `transform` method on any data you feed it. That is you `fit` your transformer object with your training data, and then of course `transform` that same data and any future data points you might have before feeding into your ML algorithm of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X)  # Train the transformer object so it knows what means and variances to use\n",
    "X_transformed = scaler.transform(X)  # transform your training data in preparation for some badass ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[0], X_transformed[0]  # An example of the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing The Cost Function\n",
    "*To understand the cost function $J(\\theta)$ better, you will now plot the cost over\n",
    "a 2-dimensional grid of $\\theta_0$ and $\\theta_1$ values.*\n",
    "\n",
    "We'll need to code the linear model, but to actually calculate the sum of squared errors (least squares loss) we can borrow a piece of code from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = df[\"popn\"]\n",
    "yvals = df[\"profit\"]\n",
    "def predict_y(thetas, xs):\n",
    "    ''' Predict y from a first order linear model in x.'''\n",
    "    ys = thetas[0] + thetas[1]*xs\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [this](http://stackoverflow.com/a/9170879/4639070) excellent stack overflow answer for a quick example of surface plots in matplotlib. We're basically copy/pasting what that guy did. The `meshgrid` function takes two vectors and sort of creates the space of that is their outer product. It is returning a matrix for $\\theta_0$ which tells us at each point on a grid (outer product) the value it takes - and the same kind of matrix for $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th0 = np.arange(-15.0, 15.0, 0.5)\n",
    "th1 = np.arange(-1, 5, 0.5)\n",
    "TH0, TH1 = np.meshgrid(th0, th1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = np.array([mse(yvals, predict_y([th0, th1], xvals)) for th0, th1 in zip(np.ravel(TH0), np.ravel(TH1))])\n",
    "Js = Js.reshape(TH0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(TH0, TH1, Js)\n",
    "ax.set_xlabel(\"theta0\")\n",
    "ax.set_ylabel(\"theta1\")\n",
    "ax.set_zlabel(\"J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "CS = plt.contour(TH0, TH1, Js)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title('Simplest default with labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Linear Regression to fit the typical linear hypothesis form can be done with `SGDRegressor` wherein you specify the specific loss function and penalty and it uses stochastic gradient descent (SGD) to do the fitting. In SGD you repeatedly run through the training set one data point at a a time and update the parameters according to the gradient of the error with respect to each individual data point. Be sure to set the $n_{\\textrm{iter}}$ parameter high enough to get good convergence. The class `LinearRegression` is less general and always uses the least squares loss, and it uses the analtical Normal Equation method for solving the fit. In all cases the tool `StandardScaler` automates the feature scaling (by mean and variance) for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
